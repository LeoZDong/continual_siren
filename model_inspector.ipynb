{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Trained Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import yaml\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import data\n",
    "from networks import *\n",
    "from utils import mse2psnr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select model to inspect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_name = \"non_continual\"\n",
    "# exp_name = \"continual_base\"\n",
    "# exp_name = \"l1/l1_1e-4\"\n",
    "# exp_name = \"sharpen_new/sharpen_first_log_sq_fq125_cur\"\n",
    "# exp_name = \"ewc/ewc_10000\"\n",
    "# exp_name = \"prune/prune_0.1_fq25\"\n",
    "exp_name = \"hash/hash_continual\"\n",
    "\n",
    "date = \"2023.01.31\"\n",
    "ids = [dir for dir in os.listdir(f\"exp/{exp_name}/{date}\") if not dir.startswith(\".\")]\n",
    "\n",
    "# Take the first ID under this date\n",
    "id = ids[0]\n",
    "# Or specify ID\n",
    "# id = \"234224\"\n",
    "\n",
    "path = f\"exp/{exp_name}/{date}/{id}\"\n",
    "print(f\"Evaluating model at path: {path}\")\n",
    "\n",
    "config_path = f\"{path}/.hydra/config.yaml\"\n",
    "ckpt_path = f\"{path}/ckpt/final.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "with open(config_path, \"r\") as stream:\n",
    "    config = yaml.safe_load(stream)\n",
    "    cfg = DictConfig(config)\n",
    "\n",
    "# Load model\n",
    "model = instantiate(cfg.network)\n",
    "ckpt = torch.load(ckpt_path)\n",
    "model_state_dict = ckpt[\"model_state_dict\"]\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "# Load source image\n",
    "dataset = instantiate(cfg.data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model_to_eval, eval_coords, show_img_out):\n",
    "    if eval_coords == \"full\":\n",
    "        model_input = dataset.full_coords\n",
    "        ground_truth = dataset.full_pixels\n",
    "    else:\n",
    "        model_input = dataset.coords_regions[eval_coords]\n",
    "        ground_truth = dataset.pixels_regions[eval_coords]\n",
    "\n",
    "    model_output = model_to_eval(model_input)[0]\n",
    "    side_length = int(math.sqrt(model_output.shape[0]))\n",
    "\n",
    "    mse = ((model_output - ground_truth) ** 2).mean().item()\n",
    "    psnr = mse2psnr(mse)\n",
    "\n",
    "    # Recover spatial dimension for visualization\n",
    "    img_out = (\n",
    "        model_output.cpu().view(side_length, side_length, -1).detach()\n",
    "    )\n",
    "\n",
    "    # Clamp image in [0, 1] for visualization\n",
    "    img_out = torch.clip(img_out, 0, 1)\n",
    "    if show_img_out:\n",
    "        fig = plt.figure()\n",
    "        plt.imshow(img_out)\n",
    "\n",
    "    return mse, psnr, img_out\n",
    "\n",
    "mse, psnr, img_out = eval(model, eval_coords=\"full\", show_img_out=False)\n",
    "print(f\"full_mse={mse}\")\n",
    "print(f\"full_psnr={psnr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in range(dataset.num_regions):\n",
    "    _, psnr, _ = eval(model, eval_coords=region, show_img_out=False)\n",
    "    print(f\"psnr_region{region}={psnr}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_weight_sparsity(model):\n",
    "    total_params = 0\n",
    "    sparse_params = 0\n",
    "    for param in model.parameters():\n",
    "        total_params += torch.numel(param)\n",
    "        sparse_params += torch.isclose(param, torch.zeros_like(param)).sum()\n",
    "    return sparse_params / total_params\n",
    "\n",
    "print(f\"sparsity={get_model_weight_sparsity(model)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prune Weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module_names(model):\n",
    "    module_names = []\n",
    "    for param_name in dict(model.named_parameters()).keys():\n",
    "        # Module name is everything in param name except for the last word\n",
    "        # e.g. If param name is 'net.0.linear.weight', then module name is 'net.0.linear'\n",
    "        name = '.'.join(param_name.split('.')[:-1])\n",
    "        module_names.append(name)\n",
    "    return module_names\n",
    "\n",
    "def get_prunable_params(model):\n",
    "    module_param_pairs = []\n",
    "    module_names = get_module_names(model)\n",
    "    for module_name in module_names:\n",
    "        module = dict(model.named_modules())[module_name]\n",
    "        module_param_pairs.append((module, \"weight\"))\n",
    "        module_param_pairs.append((module, \"bias\"))\n",
    "    \n",
    "    return module_param_pairs\n",
    "\n",
    "prunable_params = get_prunable_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pruning(prunable_params, finalize_pruning):\n",
    "    for module, prunable_param_name in prunable_params:\n",
    "        prune.l1_unstructured(module, prunable_param_name, amount=0.004)\n",
    "        if finalize_pruning:\n",
    "            prune.remove(module, prunable_param_name)\n",
    "\n",
    "apply_pruning(prunable_params, True)\n",
    "print(f\"sparsity (after pruning)={get_model_weight_sparsity(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse, psnr, img_out = eval(model, eval_coords=\"full\", show_img_out=False)\n",
    "print(f\"mse={mse}\")\n",
    "print(f\"psnr={psnr}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: A sparse SIREN can still reconstruct original image very well, but we cannot get such model by training with L1 penalty!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "img",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "810cda73fbfb906e48b314f5f0f39d6e47df735439d748866f24116813ff86d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
